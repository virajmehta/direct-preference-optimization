python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=sft exp_name=jeopardy_llama7b_sup gradient_accumulation_steps=4 batch_size=32 eval_batch_size=8 sample_during_eval=True pretrain=False  model.archive=trained_models/jeopardy_llama7b_sft/LATEST/policy.pt # model.fsdp_policy_mp=bfloat16 trainer=FSDPTrainer
# python -u train_qlora.py model=llama7b datasets=[hh] loss=sft exp_name=hh_llama7b_sft gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16  sample_during_eval=True epinet=True pretrain=False # model.fsdp_policy_mp=bfloat16 trainer=FSDPTrainer
# python -u train_qlora.py model=llama7b datasets=[shp] loss=sft exp_name=shp_llama7b_sft gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 sample_during_eval=True pretrain=False # model.fsdp_policy_mp=bfloat16 trainer=FSDPTrainer
