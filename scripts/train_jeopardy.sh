python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=dpo pretrain=False loss.beta=0.1 exp_name=jeopardy_aedpo_nulltest_2 gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 sample_during_eval=True have_llm_dropout=True active=True model.archive=/home/scratch/virajm/dpo/jeopardy_sft_nulltest_2ep_2023-09-10_18-24-14_844013/LATEST/policy.pt
python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=dpo pretrain=False loss.beta=0.1 exp_name=jeopardy_us_dpo_nulltest_2 gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 sample_during_eval=True have_llm_dropout=True active=True model.archive=/home/scratch/virajm/dpo/jeopardy_sft_nulltest_2ep_2023-09-10_18-24-14_844013/LATEST/policy.pt selection_strategy="us"
python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=dpo pretrain=False loss.beta=0.1 exp_name=jeopardy_dpo_nulltest_2 gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 sample_during_eval=True have_llm_dropout=True model.archive=/home/scratch/virajm/dpo/jeopardy_sft_nulltest_2ep_2023-09-10_18-24-14_844013/LATEST/policy.pt selection_strategy="us"
python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=sft pretrain=False exp_name=jeopardy_furthur_sft_nulltest_2 gradient_accumulation_steps=2 batch_size=32 eval_batch_size=16 sample_during_eval=True have_llm_dropout=True model.archive=/home/scratch/virajm/dpo/jeopardy_sft_nulltest_2ep_2023-09-10_18-24-14_844013/LATEST/policy.pt
