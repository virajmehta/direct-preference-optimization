# python -u train_qlora.py model=llama7b datasets=[hh] loss=dpo loss.beta=0.1 model.archive=trained_models/hh_sft_llama7b/policy.pt exp_name=ae_hh_dpo_llama7b gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=false active=True pretrain=False # trainer=FSDPTrainer
python -u train_qlora.py model=llama7b datasets=[shp] loss=dpo loss.beta=0.1 model.archive=trained_models/dropout_shp_llama7b_sft_2023-08-28_10/policy.pt exp_name=shp_dpo_llama7b_dropout gradient_accumulation_steps=6 batch_size=32 eval_batch_size=32 sample_during_eval=True active=True pretrain=False have_llm_dropout=True wandb.enabled=False  do_first_eval=False
# python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=dpo loss.beta=0.1 model.archive=trained_models/anthropic_sft_llama7b/policy.pt exp_name=anthropic_dpo_llama7b gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=false  active=True pretrain=False # trainer=FSDPTrainer
