python -u train_qlora.py model=llama7b datasets=[hh] loss=dpo loss.beta=0.1 model.archive=trained_models/hh_llama7b_sft/LATEST/policy.pt exp_name=hh_dpo_llama7b gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=True
# python -u train_qlora.py model=llama7b datasets=[shp] loss=dpo loss.beta=0.1 model.archive=trained_models/shp_llama7b_sft/LATEST/policy.pt exp_name=shp_dpo_llama7b gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=True
# python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=dpo loss.beta=0.1 model.archive=trained_models/jeopardy_llama7b_sft/LATEST/policy.pt exp_name=jeopardy_dpo_llama7b gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=True
