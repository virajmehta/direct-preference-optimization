python -u train_qlora.py model=llama7b datasets=[jeopardy] loss=dpo loss.beta=0.1 model.archive=/home/scratch/vdas/dpo/jeopardy_sft_dropout_2023-09-05_00-48-07_633053/LATEST/policy.pt exp_name=jeopardy_us_llama7b_dropout gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=True active=True pretrain=False have_llm_dropout=True selection_strategy=us eval_every=2048 max_train_examples=30000
python -u train_qlora.py model=llama7b datasets=[hh] loss=dpo loss.beta=0.1 model.archive=/home/scratch/vdas/dpo/hh_llama_dropout_quantized_2023-08-30_21-48-46_866377/LATEST/policy.pt exp_name=hh_us_llama7b_dropout gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=True active=True pretrain=False have_llm_dropout=True selection_strategy=us eval_every=2048 max_train_examples=30000
python -u train_qlora.py model=llama7b datasets=[shp] loss=dpo loss.beta=0.1 model.archive=/home/scratch/vdas/dpo/shp_sft_dropout_2023-09-05_11-15-24_310479/LATEST/policy.pt exp_name=shp_us_llama7b_dropout gradient_accumulation_steps=4 batch_size=32 eval_batch_size=32 sample_during_eval=True active=True pretrain=False have_llm_dropout=True selection_strategy=us n_epochs=2 eval_every=2048  max_train_examples=30000
